{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/fastcore/foundation.py:55: UserWarning: `patch_property` is deprecated and will be removed; use `patch(as_prop=True)` instead\n",
      "  warnings.warn(\"`patch_property` is deprecated and will be removed; use `patch(as_prop=True)` instead\")\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import array, nn, os, PILImage, torch, TensorImage\n",
    "from torchvision.transforms import ToPILImage\n",
    "from typing import List\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_conv_norm_relu(ch_in:int, ch_out:int, pad_mode:str, norm_layer:nn.Module, ks:int=3, bias:bool=True, \n",
    "                       pad=1, stride:int=1, activ:bool=True, init=nn.init.kaiming_normal_)->List[nn.Module]:\n",
    "    layers = []\n",
    "    if pad_mode == 'reflection': layers.append(nn.ReflectionPad2d(pad))\n",
    "    elif pad_mode == 'border':   layers.append(nn.ReplicationPad2d(pad))\n",
    "    p = pad if pad_mode == 'zeros' else 0\n",
    "    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=p, stride=stride, bias=bias)\n",
    "    if init:\n",
    "        init(conv.weight)\n",
    "        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)\n",
    "    layers += [conv, norm_layer(ch_out)]\n",
    "    if activ: layers.append(nn.ReLU(inplace=True))\n",
    "    return layers\n",
    "\n",
    "def convT_norm_relu(ch_in:int, ch_out:int, norm_layer:nn.Module, ks:int=3, stride:int=2, bias:bool=True):\n",
    "    return [nn.ConvTranspose2d(ch_in, ch_out, kernel_size=ks, stride=stride, padding=1, output_padding=1, bias=bias),\n",
    "            norm_layer(ch_out), nn.ReLU(True)]\n",
    "\n",
    "def convT_norm_relu(ch_in:int, ch_out:int, norm_layer:nn.Module, ks:int=3, stride:int=2, bias:bool=True):\n",
    "    return [nn.ConvTranspose2d(ch_in, ch_out, kernel_size=ks, stride=stride, padding=1, output_padding=1, bias=bias),\n",
    "            norm_layer(ch_out), nn.ReLU(True)]\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim:int, pad_mode:str='reflection', norm_layer:nn.Module=None, dropout:float=0., bias:bool=True):\n",
    "        super().__init__()\n",
    "        assert pad_mode in ['zeros', 'reflection', 'border'], f'padding {pad_mode} not implemented.'\n",
    "        norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)\n",
    "        layers = pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias)\n",
    "        if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "        layers += pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias, activ=False)\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): return x + self.conv_block(x)\n",
    "\n",
    "    \n",
    "def resnet_generator(ch_in:int, ch_out:int, n_ftrs:int=64, norm_layer:nn.Module=None, \n",
    "                     dropout:float=0., n_blocks:int=6, pad_mode:str='reflection')->nn.Module:\n",
    "    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)\n",
    "    bias = (norm_layer == nn.InstanceNorm2d)\n",
    "    layers = pad_conv_norm_relu(ch_in, n_ftrs, 'reflection', norm_layer, pad=3, ks=7, bias=bias)\n",
    "    for i in range(2):\n",
    "        layers += pad_conv_norm_relu(n_ftrs, n_ftrs *2, 'zeros', norm_layer, stride=2, bias=bias)\n",
    "        n_ftrs *= 2\n",
    "    layers += [ResnetBlock(n_ftrs, pad_mode, norm_layer, dropout, bias) for _ in range(n_blocks)]\n",
    "    for i in range(2):\n",
    "        layers += convT_norm_relu(n_ftrs, n_ftrs//2, norm_layer, bias=bias)\n",
    "        n_ftrs //= 2\n",
    "    layers += [nn.ReflectionPad2d(3), nn.Conv2d(n_ftrs, ch_out, kernel_size=7, padding=0), nn.Tanh()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv_norm_lr(ch_in:int, ch_out:int, norm_layer:nn.Module=None, ks:int=3, bias:bool=True, pad:int=1, stride:int=1, \n",
    "                 activ:bool=True, slope:float=0.2, init=nn.init.kaiming_normal_)->List[nn.Module]:\n",
    "    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=pad, stride=stride, bias=bias)\n",
    "    if init:\n",
    "        init(conv.weight)\n",
    "        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)\n",
    "    layers = [conv]\n",
    "    if norm_layer is not None: layers.append(norm_layer(ch_out))\n",
    "    if activ: layers.append(nn.LeakyReLU(slope, inplace=True))\n",
    "    return layers\n",
    "\n",
    "\n",
    "\n",
    "def discriminator(ch_in:int, n_ftrs:int=64, n_layers:int=3, norm_layer:nn.Module=None, sigmoid:bool=False)->nn.Module:\n",
    "    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)\n",
    "    bias = (norm_layer == nn.InstanceNorm2d)\n",
    "    layers = conv_norm_lr(ch_in, n_ftrs, ks=4, stride=2, pad=1)\n",
    "    for i in range(n_layers-1):\n",
    "        new_ftrs = 2*n_ftrs if i <= 3 else n_ftrs\n",
    "        layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=2, pad=1, bias=bias)\n",
    "        n_ftrs = new_ftrs\n",
    "    new_ftrs = 2*n_ftrs if n_layers <=3 else n_ftrs\n",
    "    layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=1, pad=1, bias=bias)\n",
    "    layers.append(nn.Conv2d(new_ftrs, 1, kernel_size=4, stride=1, padding=1))\n",
    "    if sigmoid: layers.append(nn.Sigmoid())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "class CycleGAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch_in:int, ch_out:int, n_features:int=64, disc_layers:int=3, gen_blocks:int=6, lsgan:bool=True, \n",
    "                 drop:float=0., norm_layer:nn.Module=None):\n",
    "        super().__init__()\n",
    "        self.D_A = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)\n",
    "        self.D_B = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)\n",
    "        self.G_A = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)\n",
    "        self.G_B = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)\n",
    "        #G_A: takes real input B and generates fake input A\n",
    "        #G_B: takes real input A and generates fake input B\n",
    "        #D_A: trained to make the difference between real input A and fake input A\n",
    "        #D_B: trained to make the difference between real input B and fake input B\n",
    "    \n",
    "    def forwardOriginal(self, real_A, real_B):\n",
    "        fake_A, fake_B = self.G_A(real_B), self.G_B(real_A)\n",
    "        if not self.training: return torch.cat([fake_A[:,None],fake_B[:,None]], 1)\n",
    "        idt_A, idt_B = self.G_A(real_A), self.G_B(real_B) #Needed for the identity loss during training.\n",
    "        return [fake_A, fake_B, idt_A, idt_B]\n",
    "    # lil fix \n",
    "    def forward(self, x):\n",
    "        real_A, real_B = x\n",
    "        fake_A, fake_B = self.G_A(real_B), self.G_B(real_A)\n",
    "        if not self.training: return torch.cat([fake_A[:,None],fake_B[:,None]], 1)\n",
    "        idt_A, idt_B = self.G_A(real_A), self.G_B(real_B) #Needed for the identity loss during training.\n",
    "        return [fake_A, fake_B, idt_A, idt_B]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('h2s.pkl')\n",
    "\n",
    "def simpsonize(img):\n",
    "    print(type(img))\n",
    "    timg = TensorImage(array(img)).permute(2,0,1).float()/255.\n",
    "    xb =  TensorImage(timg[None].expand(1, *timg.shape).clone())\n",
    "    preds = preds = (model(xb)/2 + 0.5)\n",
    "    return ToPILImage()(preds[0])\n",
    "\n",
    "im_in = gr.inputs.Image(label='Your Image')\n",
    "im_out = gr.outputs.Image(label='Simpsonized')\n",
    "\n",
    "grin = gr.Interface(fn=simpsonize, inputs=im_in, outputs=im_out, \n",
    "                    title='Simpsonizer', allow_flagging=False,\n",
    "                    show_tips=False)\n",
    "grin.launch(inline=True, inbrowser=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
