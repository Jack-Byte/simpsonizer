{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import array, nn, os, PILImage, torch, TensorImage\n",
    "from torchvision.transforms import ToPILImage\n",
    "from typing import List\n",
    "import time\n",
    "from IPython.core.display import HTML\n",
    "from ipywidgets import Label, Button, FileUpload, Output, VBox, AppLayout, Layout, Dropdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#set simpsons like font in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css2?family=Gochi+Hand&display=swap');\n",
       ".out_style{\n",
       "    color: black;\n",
       "    background-color:yellow;\n",
       "    font-family: 'Gochi Hand', cursive;\n",
       "}\n",
       ".gochihand {\n",
       "    font-family: 'Gochi Hand', cursive;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=Gochi+Hand&display=swap');\n",
    ".out_style{\n",
    "    color: black;\n",
    "    background-color:yellow;\n",
    "    font-family: 'Gochi Hand', cursive;\n",
    "}\n",
    ".gochihand {\n",
    "    font-family: 'Gochi Hand', cursive;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions used in the model\n",
    "def pad_conv_norm_relu(ch_in:int, ch_out:int, pad_mode:str, norm_layer:nn.Module, ks:int=3, bias:bool=True, \n",
    "                       pad=1, stride:int=1, activ:bool=True, init=nn.init.kaiming_normal_)->List[nn.Module]:\n",
    "    layers = []\n",
    "    if pad_mode == 'reflection': layers.append(nn.ReflectionPad2d(pad))\n",
    "    elif pad_mode == 'border':   layers.append(nn.ReplicationPad2d(pad))\n",
    "    p = pad if pad_mode == 'zeros' else 0\n",
    "    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=p, stride=stride, bias=bias)\n",
    "    if init:\n",
    "        init(conv.weight)\n",
    "        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)\n",
    "    layers += [conv, norm_layer(ch_out)]\n",
    "    if activ: layers.append(nn.ReLU(inplace=True))\n",
    "    return layers\n",
    "\n",
    "def convT_norm_relu(ch_in:int, ch_out:int, norm_layer:nn.Module, ks:int=3, stride:int=2, bias:bool=True):\n",
    "    return [nn.ConvTranspose2d(ch_in, ch_out, kernel_size=ks, stride=stride, padding=1, output_padding=1, bias=bias),\n",
    "            norm_layer(ch_out), nn.ReLU(True)]\n",
    "\n",
    "def convT_norm_relu(ch_in:int, ch_out:int, norm_layer:nn.Module, ks:int=3, stride:int=2, bias:bool=True):\n",
    "    return [nn.ConvTranspose2d(ch_in, ch_out, kernel_size=ks, stride=stride, padding=1, output_padding=1, bias=bias),\n",
    "            norm_layer(ch_out), nn.ReLU(True)]\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim:int, pad_mode:str='reflection', norm_layer:nn.Module=None, dropout:float=0., bias:bool=True):\n",
    "        super().__init__()\n",
    "        assert pad_mode in ['zeros', 'reflection', 'border'], f'padding {pad_mode} not implemented.'\n",
    "        norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)\n",
    "        layers = pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias)\n",
    "        if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "        layers += pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias, activ=False)\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): return x + self.conv_block(x)\n",
    "\n",
    "    \n",
    "def resnet_generator(ch_in:int, ch_out:int, n_ftrs:int=64, norm_layer:nn.Module=None, \n",
    "                     dropout:float=0., n_blocks:int=6, pad_mode:str='reflection')->nn.Module:\n",
    "    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)\n",
    "    bias = (norm_layer == nn.InstanceNorm2d)\n",
    "    layers = pad_conv_norm_relu(ch_in, n_ftrs, 'reflection', norm_layer, pad=3, ks=7, bias=bias)\n",
    "    for i in range(2):\n",
    "        layers += pad_conv_norm_relu(n_ftrs, n_ftrs *2, 'zeros', norm_layer, stride=2, bias=bias)\n",
    "        n_ftrs *= 2\n",
    "    layers += [ResnetBlock(n_ftrs, pad_mode, norm_layer, dropout, bias) for _ in range(n_blocks)]\n",
    "    for i in range(2):\n",
    "        layers += convT_norm_relu(n_ftrs, n_ftrs//2, norm_layer, bias=bias)\n",
    "        n_ftrs //= 2\n",
    "    layers += [nn.ReflectionPad2d(3), nn.Conv2d(n_ftrs, ch_out, kernel_size=7, padding=0), nn.Tanh()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv_norm_lr(ch_in:int, ch_out:int, norm_layer:nn.Module=None, ks:int=3, bias:bool=True, pad:int=1, stride:int=1, \n",
    "                 activ:bool=True, slope:float=0.2, init=nn.init.kaiming_normal_)->List[nn.Module]:\n",
    "    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=pad, stride=stride, bias=bias)\n",
    "    if init:\n",
    "        init(conv.weight)\n",
    "        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)\n",
    "    layers = [conv]\n",
    "    if norm_layer is not None: layers.append(norm_layer(ch_out))\n",
    "    if activ: layers.append(nn.LeakyReLU(slope, inplace=True))\n",
    "    return layers\n",
    "\n",
    "\n",
    "\n",
    "def discriminator(ch_in:int, n_ftrs:int=64, n_layers:int=3, norm_layer:nn.Module=None, sigmoid:bool=False)->nn.Module:\n",
    "    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)\n",
    "    bias = (norm_layer == nn.InstanceNorm2d)\n",
    "    layers = conv_norm_lr(ch_in, n_ftrs, ks=4, stride=2, pad=1)\n",
    "    for i in range(n_layers-1):\n",
    "        new_ftrs = 2*n_ftrs if i <= 3 else n_ftrs\n",
    "        layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=2, pad=1, bias=bias)\n",
    "        n_ftrs = new_ftrs\n",
    "    new_ftrs = 2*n_ftrs if n_layers <=3 else n_ftrs\n",
    "    layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=1, pad=1, bias=bias)\n",
    "    layers.append(nn.Conv2d(new_ftrs, 1, kernel_size=4, stride=1, padding=1))\n",
    "    if sigmoid: layers.append(nn.Sigmoid())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "class CycleGAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch_in:int, ch_out:int, n_features:int=64, disc_layers:int=3, gen_blocks:int=6, lsgan:bool=True, \n",
    "                 drop:float=0., norm_layer:nn.Module=None):\n",
    "        super().__init__()\n",
    "        self.D_A = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)\n",
    "        self.D_B = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)\n",
    "        self.G_A = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)\n",
    "        self.G_B = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)\n",
    "        #G_A: takes real input B and generates fake input A\n",
    "        #G_B: takes real input A and generates fake input B\n",
    "        #D_A: trained to make the difference between real input A and fake input A\n",
    "        #D_B: trained to make the difference between real input B and fake input B\n",
    "    \n",
    "    def forward(self, x):\n",
    "        real_A, real_B = x\n",
    "        fake_A, fake_B = self.G_A(real_B), self.G_B(real_A)\n",
    "        if not self.training: return torch.cat([fake_A[:,None],fake_B[:,None]], 1)\n",
    "        idt_A, idt_B = self.G_A(real_A), self.G_B(real_B) #Needed for the identity loss during training.\n",
    "        return [fake_A, fake_B, idt_A, idt_B]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf270d7fc741490389f20dc2d23d2929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Simpsonizer', disabled=True, layout=Layout(height='auto', width='auto'), stâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "h2s = torch.load('h2s.pkl')\n",
    "s2h = torch.load('s2h.pkl')\n",
    "\n",
    "def transform(img, kind='simpsonize'):\n",
    "    timg = TensorImage(array(img)).permute(2,0,1).float()/255.\n",
    "    #timg = tfms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(TensorImage(array(img)).permute(2,0,1).float()/255.)\n",
    "    xb =  TensorImage(timg[None].expand(1, *timg.shape).clone())\n",
    "    if kind=='simpsonize':\n",
    "        preds = (h2s(xb)/2 + 0.5)\n",
    "    else:\n",
    "        preds = (s2h(xb)/2 + 0.5)\n",
    "    return ToPILImage()(preds[0])\n",
    "\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "TEXTS = {\"btn_header\" : {\"en\" : \"Simpsonizer\", \"de\" : \"Simpsonizer\"},\n",
    "         \"dpd_lang\" : {\"en\" : \"Language\", \"de\" : \"Sprache\"},\n",
    "         \"dpd_kind\" : {\"simpsonize\" : \"Simpsonize\", \"humanize\" : \"Humanize\"},\n",
    "         \"btn_doc\" : {\"en\" : \"Show Info\", \"de\" : \"Info Anzeigen\"},\n",
    "         \"btn_upload\" : {\"en\" : \"Upload Image\", \"de\" : \"Bild hochladen\"},\n",
    "         \"btn_status_init\" : {\"en\" : \"\", \"de\" : \"\"},\n",
    "         \"btn_status_progress\" : {\"en\" : \"Please Wait... - \", \"de\" : \"Bitte Warten... - \"},\n",
    "         \"btn_status_default\" : {\"en\" : \"Please Wait\", \"de\" : \"Bitte Warten\"},\n",
    "         \"btn_status_load\" : {\"en\" : \"Loading Image\", \"de\" : \"Bild wird geladen\"},\n",
    "         \"btn_status_detect\" : {\"en\" : \"Generating simpsonized image\", \"de\" : \"Simpsonierte Version wird generiert\"},\n",
    "         \"btn_status_ready\" : {\"en\" : \"Ready For User Input\", \"de\" : \"Bereit fÃ¼r Benutzereingabe\"},\n",
    "         \"btn_status_error\" : {\"en\" : \"Error Loading Image\", \"de\" : \"Fehler beim Abruf des Bildes\"},\n",
    "         \"select_image\" : {\"en\" : \"Please Select An Valid Image File\", \"de\" : \"Bitte wÃ¤hlen Sie eine gÃ¼ltige Bilddatei\"},\n",
    "         \"prob\" : {\"en\" : \"Probability\", \"de\" : \"Wahrscheinlichkeit\"}\n",
    "        }\n",
    "\n",
    "lang=\"en\"\n",
    "kind=\"simpsonize\"\n",
    "\n",
    "HTML_EN = \"\"\"<div class=\"jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput \" data-mime-type=\"text/markdown\">\n",
    "<h1 id=\"simpsonizer\">Simpsonizer</h1>\n",
    "<p>Upload a picture and see what the simpsonized version of that image looks like.</p>\n",
    "<p>This is a small project as a result of Tanishq Abraham&#39;s <a href=\"https://github.com/tmabraham/UPIT\">UPIT</a> code and the book <a href=\"https://www.amazon.de/Deep-Learning-Coders-Fastai-Pytorch/dp/1492045527\">Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD</a>.</p>\n",
    "<p>An artificial intelligence model (CycleGAN) was trained with images of humans and Simpsons to turn a picture of a human into a Simpsons character and vice versa. The results ranged from &quot;it&#39;s something&quot; to &quot;abstract art&quot; to &quot;nightmare&quot;.</p>\n",
    "</div>\"\"\"\n",
    "\n",
    "HTML_DE = \"\"\"<div class=\"jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput \" data-mime-type=\"text/markdown\">\n",
    "<h1 id=\"simpsonizer\">Simpsonizer</h1>\n",
    "<p>Lade ein Bild hoch und finde heraus, wie die Simpson-Version davon aussieht. Nach circa 1,5 Minuten erscheint das generierte Bild. </p>\n",
    "<p>Eine KÃ¼nstliche Intelligenz Modell (CycleGAN) wurde mit Bilden von Menschen und Simpsons trainiert, um aus einem Bild eines Menschen ein Simpsons Character zu machen und umgekehrt. Die Bandbreite der Ergebnisse reicht von &quot;schÃ¶n ist anders&quot;, Ã¼ber &quot;sieht aus wie abstrakte Kunst&quot; bis hin zu &quot;Albtraum&quot;.</p>\n",
    "<p>Dies welches auf Tanishq Abraham&#39;s <a href=\"https://github.com/tmabraham/UPIT\">UPIT</a> und dem Buch <a href=\"https://www.amazon.de/Deep-Learning-Coders-Fastai-Pytorch/dp/1492045527\">Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD</a> basiert.</p>\n",
    "</div>\"\"\"\n",
    "\n",
    "DOC = {\"en\" : HTML_EN,\n",
    "       \"de\" : HTML_DE}\n",
    "\n",
    "# defining widgets\n",
    "dpd_lang = Dropdown(options=['en', 'de'], value='en', \n",
    "                    description=TEXTS[\"dpd_lang\"][lang], layout=Layout(height='auto', width='auto'))\n",
    "dpd_kind = Dropdown(options=['simpsonize', 'humanize'], value='simpsonize', \n",
    "                    description=TEXTS[\"dpd_kind\"][kind], layout=Layout(height='auto', width='auto'))\n",
    "btn_doc = Button(description=TEXTS[\"btn_doc\"][lang], layout=Layout(height='auto', width='auto'))\n",
    "btn_upload = FileUpload(description=TEXTS[\"btn_upload\"][lang], multiple=False, layout=Layout(height='auto', width='auto'))\n",
    "btn_header = Button(description=TEXTS[\"btn_header\"][lang], disabled=True, layout=Layout(height='auto', width='auto'))\n",
    "btn_status = Button(description=TEXTS[\"btn_status_init\"][lang], disabled=True, layout=Layout(height='auto', width='auto'))\n",
    "input_img = Output(clear_output=True)\n",
    "output = Output(clear_output=True)\n",
    "\n",
    "# styling\n",
    "for btn in [btn_header, output]:\n",
    "    btn.add_class('out_style')\n",
    "for btn in [btn_status, btn_doc, btn_upload, dpd_lang, dpd_kind]:\n",
    "    btn.add_class('gochihand')\n",
    "\n",
    "\n",
    "# defining event functions\n",
    "def displayWaitMessage(message=TEXTS[\"btn_status_default\"][lang]):\n",
    "    btn_status.description = f'{TEXTS[\"btn_status_progress\"][lang]} {message}'\n",
    "    btn_status.style.button_color = 'orange'\n",
    "\n",
    "def displayReadyness():\n",
    "    btn_status.description = TEXTS[\"btn_status_ready\"][lang]\n",
    "    btn_status.style.button_color = 'lightgreen'\n",
    "\n",
    "def outputImage(img):\n",
    "    with input_img:\n",
    "        output.clear_output()\n",
    "        displayWaitMessage(TEXTS[\"btn_status_load\"][lang])\n",
    "        start = time.time()\n",
    "        if VERBOSE: print(img.size)\n",
    "        display(img.to_thumb(500))\n",
    "        displayWaitMessage(TEXTS[\"btn_status_detect\"][lang])\n",
    "        end = time.time()\n",
    "        if VERBOSE: print('took', end-start, 'for displaying the image')\n",
    "        start = time.time()\n",
    "    with output:\n",
    "        gen_img = transform(img.to_thumb(500), kind=kind)\n",
    "        display(gen_img)\n",
    "        end = time.time()\n",
    "        if VERBOSE: print('took', end-start, 'for displaying the preds')\n",
    "        displayReadyness()\n",
    "\n",
    "\n",
    "def on_btn_doc_clicked(b):\n",
    "    output.clear_output()\n",
    "    input_img.clear_output()\n",
    "    with output:\n",
    "        display(HTML(DOC[lang]))\n",
    "        \n",
    "def on_data_change(change):\n",
    "    output.clear_output()\n",
    "    input_img.clear_output()\n",
    "    start = time.time()\n",
    "    img = PILImage.create(btn_upload.data[-1])\n",
    "    end = time.time()\n",
    "    if VERBOSE: print('took', end-start, 'for loading the image')\n",
    "    outputImage(img)\n",
    "    btn_upload._counter = 0\n",
    "    \n",
    "def on_lang_select(change):\n",
    "    global lang \n",
    "    lang = dpd_lang.value\n",
    "    dpd_lang.description=TEXTS[\"dpd_lang\"][lang]\n",
    "    btn_upload.description=TEXTS[\"btn_upload\"][lang]\n",
    "    btn_header.description=TEXTS[\"btn_header\"][lang]\n",
    "    btn_status.description=TEXTS[\"btn_status_init\"][lang]\n",
    "    btn_doc.description=TEXTS[\"btn_doc\"][lang]\n",
    "    \n",
    "def on_kind_select(change):\n",
    "    global kind \n",
    "    kind = dpd_kind.value\n",
    "    dpd_kind.description=TEXTS[\"dpd_kind\"][kind]\n",
    "\n",
    "# adding events\n",
    "btn_doc.on_click(on_btn_doc_clicked)\n",
    "btn_upload.observe(on_data_change, names=['data'])\n",
    "dpd_lang.observe(on_lang_select)\n",
    "dpd_kind.observe(on_kind_select)\n",
    "\n",
    "# Layout and Style \n",
    "applayout1 = AppLayout(left_sidebar=btn_doc,\n",
    "                       center=dpd_kind,\n",
    "                       right_sidebar=dpd_lang)\n",
    "applayout2 = AppLayout(header=btn_status,\n",
    "                       left_sidebar=input_img,\n",
    "                       right_sidebar=output,\n",
    "                       footer=btn_upload)\n",
    "\n",
    "displayReadyness()\n",
    "\n",
    "display(VBox([btn_header, applayout1, applayout2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simpsonize'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
